<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="BigHand2.2M"/>
  <meta property="og:description" content="BigHand2.2M Benchmark: Hand Pose Dataset and State of the Art Analysis (CVPR 2017)"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="BigHand2.2M">
  <meta name="twitter:description" content="BigHand2.2M Benchmark: Hand Pose Dataset and State of the Art Analysis (CVPR 2017)">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>HOGraspNet</title>
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" integrity="sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
 
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>

  <style>

  .center {
  text-align: center;
  }

  .image-doc{
  padding: 0 25px;
  }

  </style>
  

</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->

                <span class="author-block">
                <a href="" target="_blank">Shanxin Yuan</a><sup>1</sup>,&nbsp;</span>

                <span class="author-block">
                <a href="" target="_blank">Qi Ye</a><sup>1</sup>,&nbsp;</span>
                  
                <span class="author-block">
                <a href="" target="_blank">Bjorn Stenger</a><sup>1</sup>,&nbsp;</span>

                <span class="author-block">
                <a href="" target="_blank">Siddhant Jain</a><sup>1</sup>,&nbsp;</span>

                <span class="author-block">
                <a href="https://sites.google.com/view/tkkim/home" target="_blank">Tae-Kyun Kim</a><sup>1</sup>
                </span>
                  
                </div>

                  <!-- <div class="is-size-5 publication-authors">
                    <span class="author-block">KAIST UVR Lab.<sup>1</sup>
                  </span>
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">KAIST CVL Lab.<sup>2</sup>
                    <br>
                    <b>ECCV 2024</b></span>
                  </div> -->

                
                  <div class="is-size-5 publication-authors">
                    <span class="author-block"> Imperial College London<sup>1</sup>
                      <br><b>CVPR 2018</b>
                    </span>
                  </div>
               
                
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="./assets/1704.02463v2.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

		      <!-- ArXiv abstract Link -->
			<!-- <span class="link-block">
			  <a href="https://arxiv.org/abs/2403.17422" target="_blank"
			  class="external-link button is-normal is-rounded is-dark">
			  <span class="icon">
			    <i class="ai ai-arxiv"></i>
			  </span>
			  <span>arXiv</span>
			</a>
		      </span> -->

                    <!-- Supplementary PDF link -->
                    <!--
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>
                -->
                <!-- Dataset link -->
                <span class="link-block">
                  <a href="https://docs.google.com/forms/d/e/1FAIpQLSfdUzsryJveYVmYhd3LdNypDEFRzyI7_llzbQDcZLqR6i0dcw/viewform?usp=sf_link"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="far fa-images"></i>
                    </span>
                    <span>Data Download</span>
                    </a>
                    </span>

                  <!-- Github link -->

                  <span class="link-block">
                    <a href="https://github.com/guiggh/hand_pose_action" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

              </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/U5gleNWjz44"
            frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
      <!--/ Paper video. -->
    </div>
  </div>
</section>

<!-- Teaser video-->
<!--
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="item">
      <video poster="" id="tree" muted loop autoplay="autoplay">
        <source src="data/fig_teaser.mp4"
        type="video/mp4">
      </video>
      <h4 class="subtitle has-text-centered">
        ü§ù <b>InterHandGen</b> generates two-hand interactions with or
        without an object using a novel cascaded diffusion. This generative prior can be incorporated into any optimization or learning methods to reduce ambiguity in an ill-posed setup.
      </h4>
   </div>
   <br><br>
  </div>
</section>
-->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In this work we study the use of 3D hand poses to recognize first-person dynamic hand actions interacting with 3D objects. Towards this goal, we collected RGB-D video sequences comprised of more than 100K frames of 45 daily hand action categories, involving 26 different objects in several hand configurations. To obtain hand pose annotations, we used our own mo-cap system that automatically infers the 3D location of each of the 21 joints of a hand model via 6 magnetic sensors and inverse kinematics. Additionally, we recorded the 6D object poses and provide 3D object models for a subset of hand-object interaction sequences. To the best of our knowledge, this is the first benchmark that enables the study of first-person hand actions with the use of 3D hand poses. We present an extensive experimental evaluation of RGB-D and pose-based action recognition by 18 baselines/state-of-the-art approaches. The impact of using appearance features, poses, and their combinations are measured, and the different training/testing protocols are evaluated. Finally, we assess how ready the 3D hand pose estimation field is when hands are severely occluded by objects in egocentric views and its influence on action recognition. From the results, we see clear benefits of using hand pose as a cue for action recognition compared to other data modalities. Our dataset and experiments can be of interest to communities of 3D hand pose estimation, 6D object pose, and robotics as well as action recognition.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{FirstPersonAction_CVPR2018,
          title={First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations},
          author={Garcia-Hernando, Guillermo and Yuan, Shanxin and Baek, Seungryul and Kim, Tae-Kyun}
          booktitle = {Proceedings of Computer Vision and Pattern Recognition ({CVPR})},
          year = {2018}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>